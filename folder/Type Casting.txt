<div>Type Casting:</div><div><br></div><div>Typecasting is a way to convert a variable from one data type to another data type. For example if you want to store a long value into a simple integer then you can type cast long to int. You can convert values from one type to another explicitly using the cast operator as follows:</div><div>(type_name) expression</div><div>Consider the following example where the cast operator causes the division of one integer variable by another to be performed as a floating-point operation:</div><div>#include &lt;stdio.h&gt;</div><div>main()</div><div>{</div><div>&nbsp; &nbsp;int sum = 17, count = 5;</div><div>&nbsp; &nbsp;double mean;</div><div><br></div><div>&nbsp; &nbsp;mean = (double) sum / count;</div><div>&nbsp; &nbsp;printf("Value of mean : %f\n", mean );</div><div><br></div><div>}</div><div><br></div><div>When the above code is compiled and executed, it produces the following result:</div><div><br></div><div>Value of mean : 3.400000</div><div><br></div><div>It should be noted here that the cast operator has precedence over division, so the value of sum is first converted to type double and finally it gets divided by count yielding a double value.</div><div><br></div><div>Type conversions can be implicit which is performed by the compiler automatically, or it can be specified explicitly through the use of the cast operator. It is considered good programming practice to use the cast operator whenever type conversions are necessary.</div><div><br></div><div>Usual Arithmetic Conversion</div><div>---------------------------</div><div><br></div><div>The usual arithmetic conversions are implicitly performed to cast their values in a common type. Compiler first performs integer promotion, if operands still have different types then they are converted to the type that appears highest.</div><div><br></div><div>The usual arithmetic conversions are not performed for the assignment operators, nor for the logical operators &amp;&amp; and ||. Let us take following example to understand the concept:</div><div><br></div><div>#include &lt;stdio.h&gt;</div><div><br></div><div>main()</div><div>{</div><div>&nbsp; &nbsp;int &nbsp;i = 17;</div><div>&nbsp; &nbsp;char c = 'c'; /* ascii value is 99 */</div><div>&nbsp; &nbsp;float sum;</div><div><br></div><div>&nbsp; &nbsp;sum = i + c;</div><div>&nbsp; &nbsp;printf("Value of sum : %f\n", sum );</div><div><br></div><div>}</div><div><br></div><div>When the above code is compiled and executed, it produces the following result:</div><div><br></div><div>Value of sum : 116.000000</div><div><br></div><div>Here it is simple to understand that first c gets converted to integer but because final value is double, so usual arithmetic conversion applies and compiler convert i and c into float and add them yielding a float result.</div>